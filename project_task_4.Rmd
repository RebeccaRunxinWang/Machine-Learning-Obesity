---
title: "Project Task 4 - Results Check In"
author: "The Incredibles"
date: "11/26/2018"
output: html_document
---

```{r warning=FALSE, message=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)
```


# Read the data 
```{r}
Obesity <- read.csv(file="obesity_full.csv", header=TRUE, sep=",")
Obesity$Underweight = NULL
Obesity$Normal.Weight = NULL
Obesity$Overweight = NULL
# Just focus on Obesity rate 
#Obesity
# Year:2016-2010
```

We notice that there are 2 predictors that don't have any values from 2014 to 2016. These 2 predictors are Fresh_Fruits_and_Vegetables_Easy and Getting_a_Safe_Place_to_Exercise. We will first perform regression analysis on data from 2010-2013 to see if these 2 predictors are siginificant. 

# Get rid of the NA values
```{r}
obesity_1013 = Obesity %>%
  filter(Year <=2013)
obesity_1013 = na.omit(obesity_1013)
#obesity_1013
```

# Fitting Multiple Line Regression
```{r}
model_ML = lm(Obese~.-Year-State, data = obesity_1013)
#summary(model_ML)
```

It seems that the 2 variables (Fresh_Fruits_and_Vegetables_Easy and Getting_a_Safe_Place_to_Exercise) are not statistically significant. Now, we will perform best subset selection to see if the 2 focal variables will be selected by the method. 

```{r warning=FALSE}
library(leaps)
regfit_full_20 = regsubsets(Obese~.-Year-State, data = obesity_1013, nvmax = 20)
reg_summary =summary(regfit_full_20)
reg_summary$rsq
adj_r2_max = which.max(reg_summary$adjr2) #8
```

By the adjusted R-Sqaure, we select the 9-variable model. 

```{r}
# See which 9 predictors made the cut
coef(regfit_full_20,9)
```

Still, the 2 above variables are not selected. Thus, we conclude that these 2 variables might not be as useful as others. And from now on, we will not get rid of these 2 predictors and only focus on the remaining 18 predictors. 

# Prepare a new dataset
```{r}
obesity = Obesity
obesity$Fresh_Fruits_and_Vegetables_Easy = NULL
obesity$Getting_a_Safe_Place_to_Exercise = NULL
obesity = na.omit(obesity)
```

# Best Subset Selection 
First of all, we have too many predictors. We first perform best subset selection on the dataset to select some significant predictors.
```{r}
library(leaps)
regfit_full_20 = regsubsets(Obese~.-Year-State, data = obesity, nvmax = 20)
reg_summary =summary(regfit_full_20)
reg_summary$rsq
adj_r2_max = which.max(reg_summary$adjr2) #10
adj_r2_max
```

By the adjusted R-Sqaure, we select 10-variable model. 
```{r}
# See which 10 predictors made the cut
coef(regfit_full_20,10)
```

Now, we narrow down to these 10 predictors. Create a new dataset containing only these selected predictors and their related predictors. For example, if econmoic_conditions_excellent is selected, it indicates that econmic conditions will influence obesity rates. Thus, economic_conditions_poor/economic_conditions_good should all be included. And also the same for the predictors representing the life evaluation index.

```{r}
obesity = obesity %>% 
  select(State, Year, Obese, Amount_of_Money_to_Spend_feeling_good, Ate_Healthy_Yesterday, Elevated_Disease_Burden, Economic_Conditions_Excellent, Economic_Conditions_Good, Economic_Conditions_Onlyfair, Economic_Conditions_Poor, Have_Diabetes_Yes, Have_High_Blood_Pressure, Life_Evaluation_Index_Struggling, Life_Evaluation_Index_Sufferingg, Life_Evaluation_Index_Thriving, Standard_of_Living_Satisfied, Visited_Dentist_Last_12_Months)
#obesity
```

With the nwe dataset, we will first split them into training and test dataset. 

```{r}
x = model.matrix(Obese~.-Year-State, obesity)[,-1]
y = obesity %>%
  select(Obese) %>%
  unlist() %>%
  as.numeric()

# The training set contains the data from 2010-2014
train = obesity %>%
  filter(Year < 2015)
# The test set contains the data from 2015-2016
test = obesity %>% 
  setdiff(train)

x_train = model.matrix(Obese~.-Year-State, train)[,-1]
x_test = model.matrix(Obese~. - Year-State, test)[,-1]

y_train = train%>%
  select(Obese) %>%
  unlist() %>%
  as.numeric()

y_test = test%>%
  select(Obese) %>%
  unlist() %>%
  as.numeric()
```

# Ridge Regression 
First, we will try ridge regression. We first use cross-validation to choose the tuning parameter $\lambda$.
```{r warning=FALSE, message=FALSE}
library(glmnet)
set.seed(1)
grid = 10^seq(10, -2, length=100)
ridge_mod = glmnet(x,y,alpha=0, lambda=grid)
# Fit ridge regression model on training data
cv.out = cv.glmnet(x_train, y_train, alpha=0)

# Select lamda that minimized training MSE
bestlam = cv.out$lambda.min
bestlam #0.00289

```

```{r}
plot(cv.out)
```

```{r}
# Use best lambda to predict test data

ridge_pred = predict(ridge_mod, s=bestlam, newx = x_test)
mean((ridge_pred - y_test)^2) #Calculate test MSE
```

The test MSE is really small. 

Let's see if we can still improve the performance. To do so, we will try PCR since we still have plenty predictors. 
```{r warning=FALSE, message=FALSE}
library(pls)
set.seed(2)
pcr_fit = pcr(Obese~. - Year-State, data = train, scale = TRUE, validation = "CV")
validationplot(pcr_fit, val.type = "MSEP")
```

We find that the lowest cross-validation error occurs when $M=11$ components are used. We compute the test MSE as follows: 
```{r}
pcr_pred = predict(pcr_fit, x_test, ncomp= 11)
mean((pcr_pred-y_test)^2)
```

The test MSE is slightly higher than the test MSE obained using ridge regression. 

Finally, we fit ridge regression on the full data set which we conclude is the best method to use. 
```{r}
out = glmnet(x,y,alpha=0) # Fit ridge regression model on full dataset
# Display coefficients using lambda chosen by CV
predict(out, type = "coefficients", s=bestlam)[1:15,]
```

